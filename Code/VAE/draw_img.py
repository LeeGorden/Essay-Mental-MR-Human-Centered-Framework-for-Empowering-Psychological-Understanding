# -*- coding: utf-8 -*-
"""
Author: LiGorden
Email: likehao1006@gmail.com
URL: https://www.linkedin.com/in/kehao-li-06a9a2235/
ResearchGate: https://www.researchgate.net/profile/Gorden-Li

Draw out the image generated by VAE to check the pattern expression quality in latent space.
t-SNE is used to visualize the location of high-dimension latent space on a 2D map.
"""
import os
import numpy as np
import torch
from PIL import Image
import random
from scipy.stats import norm
import matplotlib.pyplot as plt
from torchvision import transforms

from vae_CNN_224 import ConvVAE
from sklearn.manifold import TSNE


cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")

state = torch.load('best_model_V4.0')
model = ConvVAE()
model.load_state_dict(state)
model.eval()

# -----------------------------------------输入参数--------------------------------------------------------
latent_dim = 32
inter_dim = 256
mid_dim = (3, 9, 9)
mid_num = 1
for i in mid_dim:
    mid_num *= i
# -----------------------查看现有训练样本latent space坐标以及计算降维后latent space位置-----------------------------
root = './ImageData_t_SNE_V4.0_RelatedMoreCondensed/train'
images_path = [os.path.join(root, x) for x in os.listdir(root)]
transform = transforms.Compose([lambda x: Image.open(x).convert('RGB'),  # convert('RGB')是为了以防万一原图是RGBA四通道
                                transforms.ToTensor(), ])
location_latent = list()
for image_path in images_path:
    with torch.no_grad():
        base = transform(image_path).unsqueeze(0)
        x = model.encoder(base)
        x = x.view(1, -1)
        x = model.fc1(x)
        h = model.fc2(x)
        mu, logvar = h.chunk(2, dim=-1)
        location_latent.append(np.array(mu))
        z = model.reparameterize(mu, logvar)
        # print(z)
        z = z.squeeze(0)  # 将z转化成一位tensor
        # print(z)

location_latent = np.array(location_latent).reshape(-1, latent_dim)  # encode picture(μ of latent_dim) used to visualize

# Use t-SNE to visualize
tsne = TSNE(n_components=2, metric='euclidean', init='random', random_state=501)
location_tsne = tsne.fit_transform(location_latent)  # return tuple of location in T-SNE 2D plane

plt.figure()
plt.scatter(location_tsne[:2085, 0], location_tsne[:2085, 1], color='b')  # 0:2084 are negative samples in training set
plt.scatter(location_tsne[2085:, 0], location_tsne[2085:, 1], color='r')  # 0:2084 are negative samples in training set
plt.show()

"""
# ----------------------------------------查看生成图像的pattern-------------------------------------------------
n = 4  # 最终generate图片行数与列数
image_size = 224  # 最终generate图片pixel大小

selected = 0    

# norm.ppf正态分布分位数对应对应q
# np.linspace(0.05, 0.95, n)从分位数α=0.05~0.95等间距分位数对应p的array([a(0.05)~a(0.95)])
grid_x = norm.ppf(np.linspace(0.05, 0.95, n))
grid_y = norm.ppf(np.linspace(0.05, 0.95, n))
coll = [(selected, i) for i in range(latent_dim) if i != selected]  # 因为这里latent space维度超过2, 采取两两比较的原则

for idx, (p, q) in enumerate(coll):
    print((p, q))
    figure = np.zeros((3, image_size * n, image_size * n))  # 创建一个(3, image_size * n, image_size * n)维的数组用以作图
    for i, yi in enumerate(grid_y):
        for j, xi in enumerate(grid_x):
            t = [random.random() for i in range(latent_dim)]  # 随机生成latent_dim个(0, 1)实数
            t[p], t[q] = xi, yi
            z_sampled = torch.FloatTensor(t).unsqueeze(0)
            with torch.no_grad():
                decode = model.fcr1(model.fcr2(z_sampled))
                decode = decode.view(1, *mid_dim)
                decode = model.decoder(decode)
                decode = decode.squeeze(0)

                figure[:,
                i * image_size: (i + 1) * image_size,
                j * image_size: (j + 1) * image_size
                ] = decode
    # Save the picture
    plt.figure()
    plt.title("X: {}, Y: {}".format(p, q))
    plt.xticks([])
    plt.yticks([])
    plt.axis('off')
    plt.imshow(figure.transpose(1, 2, 0))
    plt.show()
    # plt.savefig("/VAEResult/Picture Generated/t-SNE_V3.0")
"""